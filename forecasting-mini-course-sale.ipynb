{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Forecasting Mini-Course Sales with Gradient Boosting Frameworks","metadata":{}},{"cell_type":"markdown","source":"**AIM**: To predict 2022 sales data for various fictitious learning modules from different fictitious Kaggle-branded stores in different countries with sales data pertaining to 2017-2021.","metadata":{}},{"cell_type":"markdown","source":"### Step 1: Importing Libraries & Examining the Data","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade pip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!apt-get install -y python-opengl\n!apt install xvfb -y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q pycaret\n!pip install shap","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install holidays","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --force-reinstall --no-deps numpy==1.21.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport holidays\nfrom pycaret.regression import setup, compare_models \nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.stats import uniform as randFloat\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import TimeSeriesSplit\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading in the data while parsing the 'date' column in as a datetime dtype\norig_train = pd.read_csv('/kaggle/input/playground-series-s3e19/train.csv', parse_dates = ['date'])\norig_test = pd.read_csv('/kaggle/input/playground-series-s3e19/test.csv', parse_dates = ['date'])\n\n# Looking at the training data\norig_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"orig_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"orig_train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll take a look at the monthly sales per year next","metadata":{}},{"cell_type":"code","source":"monthly_sales = orig_train.copy()\nmonthly_sales['year'] = monthly_sales['date'].dt.year\nmonthly_sales['month'] = monthly_sales['date'].dt.month\n\ngrouped_data = monthly_sales.groupby(['year', 'month'])['num_sold'].sum().reset_index()\n\nsns.set(style=\"darkgrid\")\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.lineplot(data=monthly_sales, x='month', y='num_sold', hue='year', ax=ax, errorbar=None, palette='bright')\nax.set_xticks(range(1, 13))\nax.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\nax.set_xlabel('Month')\nax.set_ylabel('Sales')\nax.set_title('Monthly Sales')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can notice a significant drop in sales around the time of Covid (April 2020). This will impact our final model performance and so we'll adjust these values in a later step.","metadata":{}},{"cell_type":"code","source":"orig_test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting a distribution of target variable, num_sold\nplt.figure(figsize=(8,4))\nsns.histplot(orig_train['num_sold'], color='b', bins=30, kde=True)\nplt.title('Number Sold Distribution')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target variable distribution is stongly right-skewed, later we'll use a log transformation to amend this.","metadata":{}},{"cell_type":"markdown","source":"### Initial Observations:\n- Our train and test sets contain a mix of categorical and numerical data.\n- Our target variable is strongly right-skewed\n- There is a significant drop in sales around the time of Covid (April 2020)","metadata":{}},{"cell_type":"markdown","source":"### Covid Sales ","metadata":{}},{"cell_type":"code","source":"monthly_sales_amended = monthly_sales.copy()\n\napril_adjust = (monthly_sales_amended['year'] == 2020) & (monthly_sales_amended['month'] == 4)\nadjustment_factor_april = 1.2\nmonthly_sales_amended.loc[april_adjust, 'num_sold'] *= adjustment_factor_april\n\nmay_adjust = (monthly_sales_amended['year'] == 2020) & (monthly_sales_amended['month'] == 5)\nadjustment_factor_may = 1.05\nmonthly_sales_amended.loc[may_adjust, 'num_sold'] *= adjustment_factor_may\n\nmonthly_sales_amended","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(25, 10))\n\n# Original Data\nplt.subplot(1, 2, 1)\ngrouped_data = monthly_sales.groupby(['year', 'month'])['num_sold'].sum().reset_index()\n\nsns.set(style=\"darkgrid\")\nax = sns.lineplot(data=monthly_sales, x='month', y='num_sold', hue='year', errorbar=None, palette='bright')\nax.set_xticks(range(1, 13))\nax.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\nax.set_xlabel('Month')\nax.set_ylabel('Sales')\nax.set_title('Monthly Sales')\n\n# Amended Data\nplt.subplot(1, 2, 2)\ngrouped_data = monthly_sales.groupby(['year', 'month'])['num_sold'].sum().reset_index()\n\nsns.set(style=\"darkgrid\")\nax = sns.lineplot(data=monthly_sales_amended, x='month', y='num_sold', hue='year', errorbar=None, palette='bright')\nax.set_xticks(range(1, 13))\nax.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\nax.set_xlabel('Month')\nax.set_ylabel('Sales')\nax.set_title('Monthly Sales (no Covid)')\n\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll continue by using the adjusted values for the covid period sales.","metadata":{}},{"cell_type":"code","source":"train_clean = orig_train.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def covid_adjustment(row):\n    april_adjustment_factor = 1.2\n    may_adjustment_factor = 1.05\n    if row['date'].year == 2020:\n        if row['date'].month == 4:  # April\n            return row['num_sold'] * april_adjustment_factor\n        elif row['date'].month == 5:  # May\n            return row['num_sold'] * may_adjustment_factor\n        else:\n            return row['num_sold']\n    else:\n        return row['num_sold']\n\n# Apply the adjustments to the num_sold column in the train_clean df\ntrain_clean['num_sold'] = train_clean.apply(covid_adjustment, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2: Combining Train & Test Datasets\n\nWe'll combine both our datasets before performing transformations on our categorical and numeric data.","metadata":{}},{"cell_type":"code","source":"# Combining the data\n\n# Isolating the target variable \ntarget_var = train_clean['num_sold']\n\n# Isolating the Id column - it isn't needed in the analysis but is for testing the predictions \nids = orig_test['id']\n\n# Dropping target_var and ids from the datasets\n\nnew_train = train_clean.drop(['id','num_sold'],axis=1)\nnew_test = orig_test.drop('id',axis=1)\n\n# Creating a new df with the combined data\ncombined_data = pd.concat([new_train, new_test], axis=0).reset_index(drop=True)\n\ncombined_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3: Identifying Holidays & Weekends\n\nHolidays and weekends are known to have an impact on the sales of many products around the world. To get the most from our model we will identify and highlight these time periods.","metadata":{}},{"cell_type":"code","source":"# Extracting datetime objects from our date column\ncombined_data['year'] = combined_data['date'].dt.year\ncombined_data['month'] = combined_data['date'].dt.month\ncombined_data['day'] = combined_data['date'].dt.day\ncombined_data['dayofweek'] = combined_data.date.dt.dayofweek","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identifying weekends\ncombined_data['is_weekend'] = combined_data['date'].dt.dayofweek.isin([5, 6]).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identifying holidays\ncountry_names = ['Argentina','Spain','Estonia','Japan','Canada']\n# Creating a blank df to hold all country holiday info\nholiday_df = pd.DataFrame()\n\n# Iterating through country_names and creating a dataframe for each country's holidays\nfor country in country_names:\n    country_hols = pd.DataFrame(holidays.country_holidays(country,years=range(2017,2023)).items(), columns=['date', 'name'])\n    country_hols['country'] = country\n    # Appending each country's holiday df to the holiday_df\n    holiday_df = pd.concat([holiday_df,country_hols],axis=0, ignore_index=True)\n    \n# Converting date column from object to dt for merge    \nholiday_df['date'] = pd.to_datetime(holiday_df['date'])\n\n# Merging the dataframes using a left join, this way we'll keep only those dates that are holidays\ncombined_final = combined_data.merge(holiday_df[['date','country']], on=['date','country'],how='left', indicator=True)\n\n# Adding a holiday column that indicates if the day is a holiday\ncombined_final['holiday'] = combined_final['_merge'].apply(lambda x: 1 if x=='both' else 0)\ncombined_final = combined_final.drop(columns='_merge', axis=1)\n\ncombined_final = combined_final[['date','country','store','product','year','month','day','dayofweek','is_weekend','holiday']]\ncombined_final","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_final_df = combined_final.copy()\n\n# Isolating our country names before we transform the country column\ncombined_final_df['country_name'] = combined_final['country']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Transformations","metadata":{}},{"cell_type":"code","source":"# Encoding Categorical Columns\nle = LabelEncoder()\n\ncombined_final_df['store'] = le.fit_transform(combined_final_df['store'])\ncombined_final_df['product'] = le.fit_transform(combined_final_df['product'])\ncombined_final_df['country'] = le.fit_transform(combined_final_df['country'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transforming cyclic features - month and day \ndef day_month_transformer(df):\n    df['month_cos'] = df['month'].apply(lambda x: np.cos(x / 12 * 2 * np.pi))\n    df['day_sin'] = df['day'].apply(lambda x: np.sin(x / 365 * 2 * np.pi))\n    df['day_cos'] = df['day'].apply(lambda x: np.cos(x / 365 * 2 * np.pi))\n    return df\n\ncombined_final_df = day_month_transformer(combined_final_df)\n\ncombined_final_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 4: Transforming the Target Variable","metadata":{}},{"cell_type":"code","source":"# Transforming the target varible with logs\n\nlog_target_var = np.log(train_clean['num_sold']) \n\n# Plotting the new log distribution of target variable against the original\nplt.figure(figsize=(20,10))\n\nplt.subplot(1,2,1)\nsns.histplot(target_var, color='b', bins=30, kde=True)\nplt.title('Number Sold Distribution')\n\nplt.subplot(1,2,2)\nsns.histplot(log_target_var, color='b', bins=30, kde=True)\nplt.title('Number Sold Log Distribution')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now have a much better distribution of our target varible and will use this log version going forward.","metadata":{}},{"cell_type":"markdown","source":"## Step 5: Splitting the Data\nNow, we'll split the data back into the original training and test sets.","metadata":{}},{"cell_type":"code","source":"final_train = combined_final_df.iloc[:136950,:]\nfinal_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_test = combined_final_df.iloc[136950:,:].reset_index(drop=True)\nfinal_test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pycaret_setup = setup(data=(pd.concat([final_train,log_target_var],axis=1)), target='num_sold')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compare_models()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 6: Training the Model\nSince catboost, xgb and lightgbm were the top performing models above, we will use them to make our predictions.","metadata":{}},{"cell_type":"code","source":"# Creating training and validation sets from our full training data to access performance\nX_train, X_val, y_train, y_val = train_test_split(final_train, log_target_var, test_size=0.3, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initializing the models\ncatboost_model = CatBoostRegressor()\nxgboost_model = XGBRegressor()\nlightgbm_model = LGBMRegressor()\n\n# Parameter distributions for RandomizedSearchCV for CBR only\ncatboost_param_dist = {\n    'learning_rate': [0.01, 0.05, 0.1],\n    'iterations': randint(100, 301),  # Random integer between 100 and 300 (inclusive)\n    'depth': randint(4, 11),          # Random integer between 4 and 10 (inclusive)\n    'l2_leaf_reg': [1, 3, 5, 7,9],\n    'subsample' : randFloat(0.05, 0.95),\n    'colsample_bylevel': randFloat(0.05, 0.95)\n    \n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a TimeSeriesSplit object \ntscv = TimeSeriesSplit(n_splits=5)\n\n# RandomizedSearchCV for CBR to determine best params\nrandom_search_catboost = RandomizedSearchCV(\n    catboost_model,\n    param_distributions=catboost_param_dist,\n    n_iter=20,\n    cv=tscv,\n    n_jobs=-1\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting the RandomizedSearchCV results to the data \nfeatures = ['country', 'store', 'product', 'year', 'month',\n       'day', 'dayofweek', 'is_weekend','month_cos', 'day_sin', 'day_cos', 'holiday']\n\nrandom_search_catboost.fit(X_train[features], y_train,verbose=0) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 7: Evaluating Model Performance","metadata":{}},{"cell_type":"code","source":"# Getting the best parameters and score for the CBR model\nbest_params_catboost = random_search_catboost.best_params_\nbest_score_catboost = random_search_catboost.best_score_\n\nprint(\"\\nBest CatBoost Score:\", best_score_catboost)\nprint(\"Best CatBoost Parameters:\", best_params_catboost)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training all of our models on the training data\nmodels = {}\n\n# CBR\nmodels[\"catboost\"] = CatBoostRegressor(**best_params_catboost, random_state=12,verbose=False)\nmodels['catboost'].fit(X_train[features], y_train, \n                       eval_set = (X_val[features], y_val), verbose = False)\n# XGB\nmodels['xgb'] = XGBRegressor()\nmodels['xgb'].fit(X_train[features], y_train)\n\n# LGBM\nmodels['lgbm'] = LGBMRegressor()\nmodels['lgbm'].fit(X_train[features],y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating the sMAPE score\n\ndef smape(A, F):\n    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\n\nprint(\"sMAPE catboost:\",smape(np.exp(y_val),\n                              np.exp(models['catboost'].predict(X_val[features]))))\nprint(\"sMAPE xgb:\",smape(np.exp(y_val),\n                              np.exp(models['xgb'].predict(X_val[features]))))\nprint(\"sMAPE lightgbm:\",smape(np.exp(y_val),\n                              np.exp(models['lgbm'].predict(X_val[features]))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our sMAPE scores for each model are low, indicating a very good performance from each of the models when predicting using our validation set.","metadata":{}},{"cell_type":"markdown","source":"## Step 8: Predictions","metadata":{}},{"cell_type":"code","source":"# Training our models for the fianl predictions\nmodels = {}\n\n\nmodels[\"catboost\"] = CatBoostRegressor(**best_params_catboost, random_state=12,verbose=False)\nmodels['catboost'].fit(final_train[features], log_target_var, \n                       verbose = False)\n\nmodels['xgb'] = XGBRegressor()\nmodels['xgb'].fit(final_train[features], log_target_var)\n\nmodels['lgbm'] = LGBMRegressor()\nmodels['lgbm'].fit(final_train[features],log_target_var)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making predictions using an ensemble of our models\npredictions = (np.exp(models['catboost'].predict(final_test[features]))*0.33+\n               np.exp(models['xgb'].predict(final_test[features]))*0.33+\n               np.exp(models['lgbm'].predict(final_test[features]))*0.33)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making our predictions a series so we can add it to a DF\npredictions =  pd.Series(predictions)\n\n# Combining our predictions with the Ids we extracted at the beginning\nfinal_predictions = pd.concat([ids,predictions], axis=1)\n\n# Renaming our columns \nfinal_predictions.columns = ['Id','num_sold']\n\n# Rounding our num_sold column values\nfinal_predictions['num_sold'] = np.round(final_predictions['num_sold'],0)\n\n# Looking at our final predictions\nfinal_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submissions = final_predictions\nsubmissions.to_csv('./submission.csv', index=False, header=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}